ModelArguments:
  model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct #meta-llama/Meta-Llama-3-70B-Instruct #
  version: llava_sign_llama_3
  tune_mm_mlp_adapter: True
TrainingArguments:
  model_max_length: 2048
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  resume_from_checkpoint: False
  save_strategy: steps
  save_steps: 200
  save_total_limit: 5
  weight_decay: 0.1
  warmup_ratio: 0.03
  dataloader_num_workers: 0
  num_train_epochs: 10
  evaluation_strategy: no
  learning_rate: 0.002
  lr_scheduler_type: cosine
  logging_steps: 1
  gradient_checkpointing: True
  double_quant: True # Compress the quantization statistics through double quantization.
  quant_type: nf4
  bits: 16 # 4, 8 to enable quantization
  lora_enable: False
  lora_r: 64
  lora_alpha: 16
  lora_dropout: 0.05
  lora_weight_path: ""
  lora_bias: "none"
  group_by_modality_length: False
SignDataArguments:
  context_window_size: 2 #number of preceding sentences
  prelude_window_size: 2 #number of sentences at the beginning of the discourse 
  data_dir: /export/fs06/xzhan138/Sign_LLaVA/mock_data/
  annotation_path:
    train: annotation.train.json
    dev: annotation.dev.json
  visual_features:
    sign2vec:
      enable_input: True
      train: sign2vec.train.h5
      dev: sign2vec.dev.h5
    mae:
      enable_input: True
      train: mae.train.h5
      dev: mae.dev.h5
    dino:
      enable_input: True
      train: dino.train.h5
      dev: dino.dev.h5
    keypoint:
      enable_input: True
      train: pose.train.h5
      dev: pose.dev.h5
SignModelArguments:
  projectors:
    sign2vec: 
      dim: 768
      projector_type: mlp2x_gelu
      pretrained_projector_ckpt: None
    mae: 
      dim: 768
      projector_type: mlp2x_gelu
      pretrained_projector_ckpt: None
    dino: 
      dim: 384
      projector_type: mlp2x_gelu
      pretrained_projector_ckpt: None
    keypoint: 
      dim: 32
      projector_type: mlp2x_gelu
      pretrained_projector_ckpt: None