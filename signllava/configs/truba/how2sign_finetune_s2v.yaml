ModelArguments:
  model_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct #meta-llama/Meta-Llama-3-70B-Instruct #
  version: llava_sign_llama_3
  tune_mm_mlp_adapter: True
TrainingArguments:
  output_dir:  /truba/scratch/msaraclar/signllava/output/
  model_max_length: 2048
  bf16: True
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 1
  # auto_find_batch_size: True
  resume_from_checkpoint: False
  do_train: True
  do_eval: True
  evaluation_strategy: steps
  save_strategy: steps
  eval_steps: 300
  save_steps: 300
  logging_steps: 300
  metric_for_best_model: eval_loss
  load_best_model_at_end: True
  save_total_limit: 5
  weight_decay: 0.1
  warmup_ratio: 0
  dataloader_num_workers: 4
  num_train_epochs: 80
  learning_rate: 0.00001
  label_smoothing_factor: 0.1
  lr_scheduler_type: cosine
  gradient_checkpointing: True
  double_quant: True # Compress the quantization statistics through double quantization.
  quant_type: nf4
  bits: 4 # 4, 8 to enable quantization
  lora_enable: True
  lora_r: 64
  lora_alpha: 16
  lora_dropout: 0.05
  lora_weight_path: ""
  lora_bias: "none"
  group_by_modality_length: False
  seed: 42
  data_seed: 42
SignDataArguments:
  use_paraphrases: False
  context_window_size: 0 #number of preceding sentences
  prelude_window_size: 0 #number of sentences at the beginning of the discourse 
  data_dir: /localscratch/msaraclar/signllava_s2v_finetune/How2Sign
  annotation_path:
    train: h2s.annotations.train.json
    dev: h2s.annotations.dev.json
  visual_features:
    sign2vec:
      enable_input: True
      train: sign2vec/finetune/sign2vec-yasl-sc-sc-64-8-d1_layer-1/metadata_sign2vec.train.json
      dev: sign2vec/finetune/sign2vec-yasl-sc-sc-64-8-d1_layer-1/metadata_sign2vec.dev.json
    mae:
      enable_input: False
      train: mae/metadata_mae.train.json
      dev: mae/metadata_mae.dev.json
    dino:
      enable_input: False
      train: dino/metadata_dino.train.json
      dev: dino/metadata_dino.dev.json
    pose:
      enable_input: False
      train: pose/metadata_pose.train.json
      dev: pose/metadata_pose.dev.json
SignModelArguments:
  projectors:
    pretrained_projector_ckpt: /truba/scratch/msaraclar/signllava/output/293230-checkpoint-55000/global_step55000/mp_rank_00_model_states.pt
    sign2vec: 
      dim: 768
      projector_type: mlp2x_gelu
    mae: 
      dim: 768
      projector_type: mlp2x_gelu
    dino: 
      dim: 1152
      projector_type: mlp2x_gelu
    pose: 
      dim: 208
      projector_type: mlp2x_gelu
SignMultiTaskArguments: # make sure the sum of sample weights is 1
  translation:
    enable: True
    sample_weight: 1
  one_word_present:
    enable: False
    sample_weight: 0
  multi_word_present:
    enable: False
    sample_weight: 0
    max_num_words: 5
  is_reversed:
    enable: False
    sample_weight: 0